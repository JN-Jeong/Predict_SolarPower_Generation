{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import random\n",
    "import os\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 태양광 발전량 데이터 불러오기와 학습 및 추론 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(x):\n",
    "    Ymd, HMS = x.split(' ')\n",
    "    H, M, S = HMS.split(':')\n",
    "    H = str(int(H)-1)\n",
    "    HMS = ':'.join([H, M, S])\n",
    "    return ' '.join([Ymd, HMS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = pd.read_csv('data/energy.csv')\n",
    "energy['time'] = energy['time'].apply(lambda x:convert_time(x)) \n",
    "energy['time']=pd.to_datetime(energy['time']) + pd.DateOffset(hours=1)\n",
    "energy['tmp time']=pd.to_datetime(energy['time']) - pd.DateOffset(hours=1) # 00:00:00 시간을 포함하기위하여\n",
    "energy[['ulsan']]=energy[['ulsan']].astype(float)\n",
    "ulsan_energy=energy[['time','ulsan','tmp time']]\n",
    "train_energy=ulsan_energy[~(ulsan_energy['tmp time'].dt.year==2021)].reset_index(drop=True)\n",
    "train_energy=train_energy[24::].reset_index(drop=True) # 2월 29일 예보 데이터가 없음\n",
    "validation_energy=ulsan_energy[ulsan_energy['tmp time'].dt.year==2021].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일기 예보 데이터 불러오기와 학습 및 추론 데이터 나누기\n",
    "\n",
    "예측일 전날 23시 예보 데이터를 이용하는 모델입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_date(x):\n",
    "    return pd.DateOffset(hours=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1111: PerformanceWarning: Adding/subtracting object-dtype array to DatetimeArray not vectorized\n",
      "  PerformanceWarning,\n"
     ]
    }
   ],
   "source": [
    "ulsan_fcst = pd.read_csv('data/ulsan_fcst_data.csv')\n",
    "ulsan_fcst['Forecast time'] = pd.to_datetime(ulsan_fcst['Forecast time'])\n",
    "ulsan_fcst['Predict time']=ulsan_fcst['Forecast time'] + ulsan_fcst['forecast'].map(to_date)\n",
    "ulsan_fcst['tmp time']=ulsan_fcst['Predict time'] - pd.DateOffset(hours=1)  # 00:00:00 시간을 포함하기위하여\n",
    "ulsan_fcst_sort=ulsan_fcst.sort_values(['Predict time', 'forecast'],ascending=[True, False])\n",
    "ulsan_fcst_remove=ulsan_fcst_sort[ulsan_fcst_sort['Forecast time'].dt.hour==23].reset_index(drop=True) # 23시 데이터만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulsan_fcst_remove=ulsan_fcst_remove[ulsan_fcst_remove['forecast']<=25] # 다음날 24시간을 다 포함하는 시간까지만 forecast 사용\n",
    "ulsan_fcst_remove=ulsan_fcst_remove[~(ulsan_fcst_remove['Forecast time'].dt.day==ulsan_fcst_remove['Predict time'].dt.day)]\n",
    "train_ulsan_fcst=ulsan_fcst_remove[~(ulsan_fcst_remove['tmp time'].dt.year==2021)].reset_index(drop=True)\n",
    "validation_ulsan_fcst=ulsan_fcst_remove[(ulsan_fcst_remove['tmp time'].dt.year==2021) & (ulsan_fcst_remove['tmp time'].dt.month==1)].reset_index(drop=True)\n",
    "test_ulsan_fcst=ulsan_fcst_remove[(ulsan_fcst_remove['tmp time'].dt.year==2021) & (ulsan_fcst_remove['tmp time'].dt.month==2)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(df, energy, target):\n",
    "    df=df.reset_index(drop=True)\n",
    "    df['Predict time']=df['Predict time'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    df['date'] = df['Predict time'].str.split(' ').str[0]\n",
    "    df['hour'] = df['Predict time'].str.split(' ').str[1].str.split(':').str[0].astype(int)\n",
    "    df['year'] = df['date'].str.split('-').str[0].astype(int)\n",
    "    df['month'] = df['date'].str.split('-').str[1].astype(int)\n",
    "    df['day'] = df['date'].str.split('-').str[2].astype(int)\n",
    "\n",
    "    train_df=df[['month', 'hour', 'Temperature','Humidity', 'WindSpeed', 'WindDirection', 'Cloud']]\n",
    "\n",
    "    train_df['Humidity']=train_df['Humidity']/100\n",
    "    train_df['WindDirection']=train_df['WindDirection']/360\n",
    "\n",
    "    for idx in range(int(len(train_df)/8)):\n",
    "        if idx == 0:\n",
    "            train=np.array(train_df.iloc[idx*8:(idx+1)*8,:]).reshape(1,8,7)\n",
    "        else:        \n",
    "            train=np.concatenate( (train, np.array(train_df.iloc[idx*8:(idx+1)*8,:]).reshape(1,8,7)),axis=0)\n",
    "\n",
    "    label=np.array(energy[target]).reshape(int(len(energy)/24),24)\n",
    "\n",
    "    return train, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Input shape : (1036, 8, 7)\n",
      "Train set Label shape : (1036, 24)\n",
      "Validation set Input shape : (31, 8, 7)\n",
      "Validation set Label shape : (31, 24)\n",
      "Test set Input shape : (28, 8, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_x, train_y= data_gen(train_ulsan_fcst,train_energy, 'ulsan')\n",
    "idx=(np.isnan(train_x).sum(axis=2).sum(axis=1))==0\n",
    "train_x=train_x[idx]\n",
    "train_y=train_y[idx]\n",
    "shuffle_idx = np.arange(len(train_x))\n",
    "np.random.shuffle(shuffle_idx)\n",
    "train_x = train_x[shuffle_idx]\n",
    "train_y = train_y[shuffle_idx]\n",
    "print('Train set Input shape : ' + str(train_x.shape))\n",
    "print('Train set Label shape : ' + str(train_y.shape))\n",
    "\n",
    "validation_x, validation_y = data_gen(validation_ulsan_fcst,validation_energy)\n",
    "print('Validation set Input shape : ' + str(validation_x.shape))\n",
    "print('Validation set Label shape : ' + str(validation_y.shape))\n",
    "\n",
    "test_x, _ = data_gen(test_ulsan_fcst,validation_energy) # test 데이터 생성\n",
    "print('Test set Input shape : ' + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Dilated causal 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_dilated_cnn(Model_input):\n",
    "    x = Conv1D(8, 3, padding='causal')(Model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(16, 3, padding='causal', dilation_rate=2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(32, 3, padding='causal', dilation_rate=4)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(64, 3, padding='causal', dilation_rate=8)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(64, 1)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(50)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Dense(24)(x)\n",
    "\n",
    "    output = Model(Model_input, x, name='regression_dilated_cnn')\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"regression_dilated_cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8, 7)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 8, 8)              176       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 8, 8)              32        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 8, 16)             400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 32)             1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 8, 64)             6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 8, 64)             4160      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                25650     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 24)                1224      \n",
      "=================================================================\n",
      "Total params: 40,066\n",
      "Trainable params: 39,726\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14317.7803\n",
      "Epoch 2/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14207.7539\n",
      "Epoch 3/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 14062.3662\n",
      "Epoch 4/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 13855.6064\n",
      "Epoch 5/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 13601.4287\n",
      "Epoch 6/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 13317.2500\n",
      "Epoch 7/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 12994.2197\n",
      "Epoch 8/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 12625.2705\n",
      "Epoch 9/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 12244.2266\n",
      "Epoch 10/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 11831.6172\n",
      "Epoch 11/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 11410.3223\n",
      "Epoch 12/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 10955.5400\n",
      "Epoch 13/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 10514.1602\n",
      "Epoch 14/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 10059.9395\n",
      "Epoch 15/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 9601.5664\n",
      "Epoch 16/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 9119.7051\n",
      "Epoch 17/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 8697.1895\n",
      "Epoch 18/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 8211.7627\n",
      "Epoch 19/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 7816.6152\n",
      "Epoch 20/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 7314.3301\n",
      "Epoch 21/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 6890.8862\n",
      "Epoch 22/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 6474.5479\n",
      "Epoch 23/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 6124.9600\n",
      "Epoch 24/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 5736.2373\n",
      "Epoch 25/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 5389.3457\n",
      "Epoch 26/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 5115.8662\n",
      "Epoch 27/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4792.0957\n",
      "Epoch 28/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4534.2793\n",
      "Epoch 29/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4244.6621\n",
      "Epoch 30/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 4014.4761\n",
      "Epoch 31/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 3764.6262\n",
      "Epoch 32/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 3509.5552\n",
      "Epoch 33/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 3374.6355\n",
      "Epoch 34/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 3184.7856\n",
      "Epoch 35/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 3071.5139\n",
      "Epoch 36/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2860.6826\n",
      "Epoch 37/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2791.2529\n",
      "Epoch 38/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2652.5259\n",
      "Epoch 39/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2470.1443\n",
      "Epoch 40/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2337.5745\n",
      "Epoch 41/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2301.5686\n",
      "Epoch 42/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2268.1335\n",
      "Epoch 43/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2143.3513\n",
      "Epoch 44/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2158.6472\n",
      "Epoch 45/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2088.9680\n",
      "Epoch 46/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 2008.6461\n",
      "Epoch 47/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1984.4333\n",
      "Epoch 48/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1821.4353\n",
      "Epoch 49/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1910.7455\n",
      "Epoch 50/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1892.0883\n",
      "Epoch 51/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1817.1014\n",
      "Epoch 52/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1809.4622\n",
      "Epoch 53/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1782.3733\n",
      "Epoch 54/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1688.6359\n",
      "Epoch 55/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1702.5106\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: 1775.5225\n",
      "Epoch 57/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1677.1484\n",
      "Epoch 58/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1686.8179\n",
      "Epoch 59/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1706.9807\n",
      "Epoch 60/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1676.0769\n",
      "Epoch 61/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1702.8636\n",
      "Epoch 62/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1579.9583\n",
      "Epoch 63/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1598.4490\n",
      "Epoch 64/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1656.3656\n",
      "Epoch 65/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1679.5183\n",
      "Epoch 66/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1660.7285\n",
      "Epoch 67/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1604.9747\n",
      "Epoch 68/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1619.2983\n",
      "Epoch 69/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1568.1343\n",
      "Epoch 70/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1557.4707\n",
      "Epoch 71/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1593.9249\n",
      "Epoch 72/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1553.9886\n",
      "Epoch 73/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1523.5721\n",
      "Epoch 74/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1526.3840\n",
      "Epoch 75/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1594.6437\n",
      "Epoch 76/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1568.5037\n",
      "Epoch 77/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1528.7896\n",
      "Epoch 78/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1523.2960\n",
      "Epoch 79/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1618.3351\n",
      "Epoch 80/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1507.5247\n",
      "Epoch 81/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1528.2161\n",
      "Epoch 82/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1559.1433\n",
      "Epoch 83/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1570.8282\n",
      "Epoch 84/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1526.5873\n",
      "Epoch 85/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1471.5767\n",
      "Epoch 86/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1584.3400\n",
      "Epoch 87/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1519.2743\n",
      "Epoch 88/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1524.7504\n",
      "Epoch 89/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1509.0297\n",
      "Epoch 90/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1474.6265\n",
      "Epoch 91/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1550.0977\n",
      "Epoch 92/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1492.9548\n",
      "Epoch 93/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1477.9902\n",
      "Epoch 94/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1516.4194\n",
      "Epoch 95/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1472.3569\n",
      "Epoch 96/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1533.2386\n",
      "Epoch 97/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1496.9037\n",
      "Epoch 98/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1467.6470\n",
      "Epoch 99/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1467.0815\n",
      "Epoch 100/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1537.0850\n",
      "Epoch 101/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1503.7523\n",
      "Epoch 102/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1459.8539\n",
      "Epoch 103/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1459.1389\n",
      "Epoch 104/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1475.0834\n",
      "Epoch 105/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1496.1332\n",
      "Epoch 106/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1492.4810\n",
      "Epoch 107/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1453.8802\n",
      "Epoch 108/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1457.2845\n",
      "Epoch 109/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1448.1180\n",
      "Epoch 110/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1479.9155\n",
      "Epoch 111/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1408.6646\n",
      "Epoch 112/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1442.5482\n",
      "Epoch 113/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1442.2262\n",
      "Epoch 114/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1478.5603\n",
      "Epoch 115/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1474.4357\n",
      "Epoch 116/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1468.6405\n",
      "Epoch 117/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1478.3854\n",
      "Epoch 118/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1482.7892\n",
      "Epoch 119/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1409.6116\n",
      "Epoch 120/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1480.0022\n",
      "Epoch 121/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1455.9817\n",
      "Epoch 122/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1467.0110\n",
      "Epoch 123/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1449.1472\n",
      "Epoch 124/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1444.0072\n",
      "Epoch 125/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1466.9637\n",
      "Epoch 126/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1416.7612\n",
      "Epoch 127/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1419.8672\n",
      "Epoch 128/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1382.0466\n",
      "Epoch 129/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1396.0707\n",
      "Epoch 130/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1432.1814\n",
      "Epoch 131/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1436.5745\n",
      "Epoch 132/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1362.6492\n",
      "Epoch 133/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1391.7804\n",
      "Epoch 134/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1417.1335\n",
      "Epoch 135/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1460.4825\n",
      "Epoch 136/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1387.6532\n",
      "Epoch 137/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1473.6073\n",
      "Epoch 138/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1445.0120\n",
      "Epoch 139/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1460.9884\n",
      "Epoch 140/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1423.8137\n",
      "Epoch 141/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1425.7842\n",
      "Epoch 142/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1422.5959\n",
      "Epoch 143/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1383.8394\n",
      "Epoch 144/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1419.4547\n",
      "Epoch 145/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1411.9062\n",
      "Epoch 146/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1442.8807\n",
      "Epoch 147/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1438.5879\n",
      "Epoch 148/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1446.4343\n",
      "Epoch 149/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1423.7391\n",
      "Epoch 150/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1428.4025\n",
      "Epoch 151/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1440.9257\n",
      "Epoch 152/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1463.6517\n",
      "Epoch 153/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: 1453.1494\n",
      "Epoch 154/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1358.1252\n",
      "Epoch 155/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1426.0092\n",
      "Epoch 156/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1396.3342\n",
      "Epoch 157/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1398.4778\n",
      "Epoch 158/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1358.6390\n",
      "Epoch 159/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1411.5227\n",
      "Epoch 160/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1442.1783\n",
      "Epoch 161/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1448.5615\n",
      "Epoch 162/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1366.0410\n",
      "Epoch 163/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1330.0743\n",
      "Epoch 164/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1459.6530\n",
      "Epoch 165/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1416.6722\n",
      "Epoch 166/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1403.9586\n",
      "Epoch 167/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1394.8829\n",
      "Epoch 168/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1464.2014\n",
      "Epoch 169/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1361.8804\n",
      "Epoch 170/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1405.6470\n",
      "Epoch 171/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1385.4735\n",
      "Epoch 172/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1340.0050\n",
      "Epoch 173/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1349.6407\n",
      "Epoch 174/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1395.6216\n",
      "Epoch 175/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1415.5658\n",
      "Epoch 176/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1387.3615\n",
      "Epoch 177/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1317.8796\n",
      "Epoch 178/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1384.5933\n",
      "Epoch 179/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1338.2797\n",
      "Epoch 180/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1376.2139\n",
      "Epoch 181/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1387.3478\n",
      "Epoch 182/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1368.0073\n",
      "Epoch 183/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1408.0363\n",
      "Epoch 184/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1365.4116\n",
      "Epoch 185/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1306.7755\n",
      "Epoch 186/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1335.5327\n",
      "Epoch 187/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1416.6709\n",
      "Epoch 188/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1414.2642\n",
      "Epoch 189/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1370.6493\n",
      "Epoch 190/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1420.3330\n",
      "Epoch 191/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 1367.2006\n",
      "Epoch 192/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1339.0309\n",
      "Epoch 193/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1357.4994\n",
      "Epoch 194/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1381.9169\n",
      "Epoch 195/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1444.1449\n",
      "Epoch 196/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1425.4440\n",
      "Epoch 197/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1362.1621\n",
      "Epoch 198/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1358.2019\n",
      "Epoch 199/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1374.5695\n",
      "Epoch 200/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 1407.1168\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18f8045b948>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = keras.Input(shape=(8, 7))\n",
    "AI_model = regression_dilated_cnn(model_inputs)\n",
    "print(AI_model.summary())\n",
    "AI_model.compile(loss='mse', optimizer='adam')\n",
    "AI_model.fit(train_x, train_y, epochs=200, batch_size=32)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=AI_model.predict(validation_x)\n",
    "prediction=np.abs(prediction)\n",
    "prediction=np.round(prediction,0)\n",
    "predict_y=np.ravel(prediction)\n",
    "valid_y=np.ravel(validation_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 울산 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = AI_model.predict(test_x) # test 데이터로 발전량 결과 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulsan_pred = pred_test.reshape(672, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 수상 태양광 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "floating_energy=energy[['time','dangjin_floating','tmp time']]\n",
    "train_energy=floating_energy[~(floating_energy['tmp time'].dt.year==2021)].reset_index(drop=True)\n",
    "train_energy=train_energy[24::].reset_index(drop=True) # 2월 29일 예보 데이터가 없음\n",
    "validation_energy=floating_energy[floating_energy['tmp time'].dt.year==2021].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1111: PerformanceWarning: Adding/subtracting object-dtype array to DatetimeArray not vectorized\n",
      "  PerformanceWarning,\n"
     ]
    }
   ],
   "source": [
    "dangjin_fcst = pd.read_csv('data/dangjin_fcst_data.csv')\n",
    "dangjin_fcst['Forecast time'] = pd.to_datetime(dangjin_fcst['Forecast time'])\n",
    "dangjin_fcst['Predict time']=dangjin_fcst['Forecast time'] + dangjin_fcst['forecast'].map(to_date)\n",
    "dangjin_fcst['tmp time']=dangjin_fcst['Predict time'] - pd.DateOffset(hours=1)  # 00:00:00 시간을 포함하기위하여\n",
    "dangjin_fcst_sort=dangjin_fcst.sort_values(['Predict time', 'forecast'],ascending=[True, False])\n",
    "dangjin_fcst_remove=dangjin_fcst_sort[dangjin_fcst_sort['Forecast time'].dt.hour==23].reset_index(drop=True) # 23시 데이터만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dangjin_fcst_remove=dangjin_fcst_remove[dangjin_fcst_remove['forecast']<=25] # 다음날 24시간을 다 포함하는 시간까지만 forecast 사용\n",
    "dangjin_fcst_remove=dangjin_fcst_remove[~(dangjin_fcst_remove['Forecast time'].dt.day==dangjin_fcst_remove['Predict time'].dt.day)]\n",
    "train_dangjin_fcst=dangjin_fcst_remove[~(dangjin_fcst_remove['tmp time'].dt.year==2021)].reset_index(drop=True)\n",
    "validation_dangjin_fcst=dangjin_fcst_remove[(dangjin_fcst_remove['tmp time'].dt.year==2021) & (dangjin_fcst_remove['tmp time'].dt.month==1)].reset_index(drop=True)\n",
    "test_dangjin_fcst=dangjin_fcst_remove[(dangjin_fcst_remove['tmp time'].dt.year==2021) & (dangjin_fcst_remove['tmp time'].dt.month==2)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Input shape : (1036, 8, 7)\n",
      "Train set Label shape : (1036, 24)\n",
      "Validation set Input shape : (31, 8, 7)\n",
      "Validation set Label shape : (31, 24)\n",
      "Test set Input shape : (28, 8, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_x, train_y= data_gen(train_dangjin_fcst,train_energy, 'dangjin_floating')\n",
    "idx=(np.isnan(train_x).sum(axis=2).sum(axis=1))==0\n",
    "train_x=train_x[idx]\n",
    "train_y=train_y[idx]\n",
    "shuffle_idx = np.arange(len(train_x))\n",
    "np.random.shuffle(shuffle_idx)\n",
    "train_x = train_x[shuffle_idx]\n",
    "train_y = train_y[shuffle_idx]\n",
    "print('Train set Input shape : ' + str(train_x.shape))\n",
    "print('Train set Label shape : ' + str(train_y.shape))\n",
    "\n",
    "validation_x, validation_y = data_gen(validation_dangjin_fcst,validation_energy, 'dangjin_floating')\n",
    "print('Validation set Input shape : ' + str(validation_x.shape))\n",
    "print('Validation set Label shape : ' + str(validation_y.shape))\n",
    "\n",
    "test_x, _ = data_gen(test_dangjin_fcst,validation_energy, 'dangjin_floating') # test 데이터 생성\n",
    "print('Test set Input shape : ' + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>dangjin_floating</th>\n",
       "      <th>tmp time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 01:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 02:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 03:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-01 04:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-01 05:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-03-01 06:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-03-01 07:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-03-01 08:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-03-01 09:00:00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2018-03-01 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-01 10:00:00</td>\n",
       "      <td>313.0</td>\n",
       "      <td>2018-03-01 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-03-01 11:00:00</td>\n",
       "      <td>532.0</td>\n",
       "      <td>2018-03-01 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-03-01 12:00:00</td>\n",
       "      <td>607.0</td>\n",
       "      <td>2018-03-01 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-03-01 13:00:00</td>\n",
       "      <td>614.0</td>\n",
       "      <td>2018-03-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-03-01 14:00:00</td>\n",
       "      <td>608.0</td>\n",
       "      <td>2018-03-01 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-03-01 15:00:00</td>\n",
       "      <td>641.0</td>\n",
       "      <td>2018-03-01 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-03-01 16:00:00</td>\n",
       "      <td>536.0</td>\n",
       "      <td>2018-03-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-03-01 17:00:00</td>\n",
       "      <td>348.0</td>\n",
       "      <td>2018-03-01 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-03-01 18:00:00</td>\n",
       "      <td>134.0</td>\n",
       "      <td>2018-03-01 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-03-01 19:00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2018-03-01 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-03-01 20:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-03-01 21:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-03-01 22:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-03-01 23:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-03-02 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-03-01 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  dangjin_floating            tmp time\n",
       "0  2018-03-01 01:00:00               0.0 2018-03-01 00:00:00\n",
       "1  2018-03-01 02:00:00               0.0 2018-03-01 01:00:00\n",
       "2  2018-03-01 03:00:00               0.0 2018-03-01 02:00:00\n",
       "3  2018-03-01 04:00:00               0.0 2018-03-01 03:00:00\n",
       "4  2018-03-01 05:00:00               0.0 2018-03-01 04:00:00\n",
       "5  2018-03-01 06:00:00               0.0 2018-03-01 05:00:00\n",
       "6  2018-03-01 07:00:00               0.0 2018-03-01 06:00:00\n",
       "7  2018-03-01 08:00:00               0.0 2018-03-01 07:00:00\n",
       "8  2018-03-01 09:00:00              36.0 2018-03-01 08:00:00\n",
       "9  2018-03-01 10:00:00             313.0 2018-03-01 09:00:00\n",
       "10 2018-03-01 11:00:00             532.0 2018-03-01 10:00:00\n",
       "11 2018-03-01 12:00:00             607.0 2018-03-01 11:00:00\n",
       "12 2018-03-01 13:00:00             614.0 2018-03-01 12:00:00\n",
       "13 2018-03-01 14:00:00             608.0 2018-03-01 13:00:00\n",
       "14 2018-03-01 15:00:00             641.0 2018-03-01 14:00:00\n",
       "15 2018-03-01 16:00:00             536.0 2018-03-01 15:00:00\n",
       "16 2018-03-01 17:00:00             348.0 2018-03-01 16:00:00\n",
       "17 2018-03-01 18:00:00             134.0 2018-03-01 17:00:00\n",
       "18 2018-03-01 19:00:00              11.0 2018-03-01 18:00:00\n",
       "19 2018-03-01 20:00:00               0.0 2018-03-01 19:00:00\n",
       "20 2018-03-01 21:00:00               0.0 2018-03-01 20:00:00\n",
       "21 2018-03-01 22:00:00               0.0 2018-03-01 21:00:00\n",
       "22 2018-03-01 23:00:00               0.0 2018-03-01 22:00:00\n",
       "23 2018-03-02 00:00:00               0.0 2018-03-01 23:00:00"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "floating_energy[:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>dangjin</th>\n",
       "      <th>tmp time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01 01:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-03-01 02:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-03-01 03:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-03-01 04:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-01 05:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018-03-01 06:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2018-03-01 07:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018-03-01 08:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018-03-01 09:00:00</td>\n",
       "      <td>37</td>\n",
       "      <td>2018-03-01 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018-03-01 10:00:00</td>\n",
       "      <td>318</td>\n",
       "      <td>2018-03-01 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2018-03-01 11:00:00</td>\n",
       "      <td>490</td>\n",
       "      <td>2018-03-01 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2018-03-01 12:00:00</td>\n",
       "      <td>550</td>\n",
       "      <td>2018-03-01 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018-03-01 13:00:00</td>\n",
       "      <td>727</td>\n",
       "      <td>2018-03-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2018-03-01 14:00:00</td>\n",
       "      <td>733</td>\n",
       "      <td>2018-03-01 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2018-03-01 15:00:00</td>\n",
       "      <td>672</td>\n",
       "      <td>2018-03-01 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2018-03-01 16:00:00</td>\n",
       "      <td>546</td>\n",
       "      <td>2018-03-01 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018-03-01 17:00:00</td>\n",
       "      <td>364</td>\n",
       "      <td>2018-03-01 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2018-03-01 18:00:00</td>\n",
       "      <td>110</td>\n",
       "      <td>2018-03-01 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2018-03-01 19:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-03-01 20:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2018-03-01 21:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018-03-01 22:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2018-03-01 23:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2018-03-02 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-03-01 23:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  dangjin            tmp time\n",
       "0  2018-03-01 01:00:00        0 2018-03-01 00:00:00\n",
       "1  2018-03-01 02:00:00        0 2018-03-01 01:00:00\n",
       "2  2018-03-01 03:00:00        0 2018-03-01 02:00:00\n",
       "3  2018-03-01 04:00:00        0 2018-03-01 03:00:00\n",
       "4  2018-03-01 05:00:00        0 2018-03-01 04:00:00\n",
       "5  2018-03-01 06:00:00        0 2018-03-01 05:00:00\n",
       "6  2018-03-01 07:00:00        0 2018-03-01 06:00:00\n",
       "7  2018-03-01 08:00:00        0 2018-03-01 07:00:00\n",
       "8  2018-03-01 09:00:00       37 2018-03-01 08:00:00\n",
       "9  2018-03-01 10:00:00      318 2018-03-01 09:00:00\n",
       "10 2018-03-01 11:00:00      490 2018-03-01 10:00:00\n",
       "11 2018-03-01 12:00:00      550 2018-03-01 11:00:00\n",
       "12 2018-03-01 13:00:00      727 2018-03-01 12:00:00\n",
       "13 2018-03-01 14:00:00      733 2018-03-01 13:00:00\n",
       "14 2018-03-01 15:00:00      672 2018-03-01 14:00:00\n",
       "15 2018-03-01 16:00:00      546 2018-03-01 15:00:00\n",
       "16 2018-03-01 17:00:00      364 2018-03-01 16:00:00\n",
       "17 2018-03-01 18:00:00      110 2018-03-01 17:00:00\n",
       "18 2018-03-01 19:00:00        0 2018-03-01 18:00:00\n",
       "19 2018-03-01 20:00:00        0 2018-03-01 19:00:00\n",
       "20 2018-03-01 21:00:00        0 2018-03-01 20:00:00\n",
       "21 2018-03-01 22:00:00        0 2018-03-01 21:00:00\n",
       "22 2018-03-01 23:00:00        0 2018-03-01 22:00:00\n",
       "23 2018-03-02 00:00:00        0 2018-03-01 23:00:00"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dangjin_energy[:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.        ,  3.        ,  7.        , ...,  3.4       ,\n",
       "          0.575     ,  4.        ],\n",
       "        [ 2.        ,  6.        ,  6.        , ...,  2.5       ,\n",
       "          0.61666667,  4.        ],\n",
       "        [ 2.        ,  9.        ,  6.        , ...,  3.3       ,\n",
       "          0.64444444,  4.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        ,  4.        , ...,  3.6       ,\n",
       "          0.85833333,  3.        ],\n",
       "        [ 2.        , 21.        ,  1.        , ...,  6.6       ,\n",
       "          0.90277778,  3.        ],\n",
       "        [ 2.        ,  0.        , -3.        , ...,  7.4       ,\n",
       "          0.92222222,  1.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        , -5.        , ...,  7.6       ,\n",
       "          0.90833333,  1.        ],\n",
       "        [ 2.        ,  6.        , -6.        , ...,  7.1       ,\n",
       "          0.90555556,  1.        ],\n",
       "        [ 2.        ,  9.        , -6.        , ...,  7.        ,\n",
       "          0.90277778,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        , -4.        , ...,  5.8       ,\n",
       "          0.84444444,  4.        ],\n",
       "        [ 2.        , 21.        , -3.        , ...,  6.2       ,\n",
       "          0.9       ,  4.        ],\n",
       "        [ 2.        ,  0.        , -4.        , ...,  5.4       ,\n",
       "          0.94444444,  4.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        , -5.        , ...,  2.8       ,\n",
       "          0.90277778,  3.        ],\n",
       "        [ 2.        ,  6.        , -6.        , ...,  0.7       ,\n",
       "          0.90555556,  1.        ],\n",
       "        [ 2.        ,  9.        , -4.        , ...,  2.2       ,\n",
       "          0.35833333,  3.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        ,  0.        , ...,  3.1       ,\n",
       "          0.51944444,  4.        ],\n",
       "        [ 2.        , 21.        ,  0.        , ...,  3.5       ,\n",
       "          0.59166667,  4.        ],\n",
       "        [ 2.        ,  0.        ,  2.        , ...,  5.9       ,\n",
       "          0.84722222,  4.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.        ,  3.        ,  0.        , ...,  2.3       ,\n",
       "          0.14444444,  1.        ],\n",
       "        [ 2.        ,  6.        ,  0.        , ...,  2.6       ,\n",
       "          0.18055556,  1.        ],\n",
       "        [ 2.        ,  9.        ,  3.        , ...,  4.1       ,\n",
       "          0.24166667,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        , 10.        , ...,  3.8       ,\n",
       "          0.04722222,  1.        ],\n",
       "        [ 2.        , 21.        ,  6.        , ...,  4.5       ,\n",
       "          0.175     ,  1.        ],\n",
       "        [ 2.        ,  0.        ,  3.        , ...,  3.1       ,\n",
       "          0.21388889,  1.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        ,  2.        , ...,  3.7       ,\n",
       "          0.19722222,  1.        ],\n",
       "        [ 2.        ,  6.        ,  1.        , ...,  3.6       ,\n",
       "          0.24444444,  1.        ],\n",
       "        [ 2.        ,  9.        ,  4.        , ...,  4.9       ,\n",
       "          0.27222222,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        , 10.        , ...,  3.7       ,\n",
       "          0.35      ,  1.        ],\n",
       "        [ 2.        , 21.        ,  5.        , ...,  3.3       ,\n",
       "          0.30833333,  1.        ],\n",
       "        [ 2.        ,  0.        ,  3.        , ...,  3.7       ,\n",
       "          0.28055556,  1.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        ,  1.        , ...,  3.3       ,\n",
       "          0.25555556,  4.        ],\n",
       "        [ 2.        ,  6.        ,  1.        , ...,  3.5       ,\n",
       "          0.325     ,  3.        ],\n",
       "        [ 2.        ,  9.        ,  4.        , ...,  3.6       ,\n",
       "          0.35277778,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        ,  9.        , ...,  0.8       ,\n",
       "          0.93611111,  4.        ],\n",
       "        [ 2.        , 21.        ,  7.        , ...,  1.3       ,\n",
       "          0.05      ,  4.        ],\n",
       "        [ 3.        ,  0.        ,  6.        , ...,  1.3       ,\n",
       "          0.33888889,  4.        ]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"regression_dilated_cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 8, 7)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 8, 8)              176       \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 8, 8)              32        \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 8, 16)             400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 8, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_37 (Conv1D)           (None, 8, 32)             1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 8, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_38 (Conv1D)           (None, 8, 64)             6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 8, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 8, 64)             4160      \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 50)                25650     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 24)                1224      \n",
      "=================================================================\n",
      "Total params: 40,066\n",
      "Trainable params: 39,726\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan   \n",
      "Epoch 2/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 3/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 4/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 5/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 12/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 15/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 18/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 21/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 23/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 33/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 34/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 37/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 56/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 59/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 62/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 64/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 65/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 67/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 73/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 76/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 78/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 79/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 81/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 87/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 93/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 102/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 103/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 107/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 113/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 115/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 118/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 127/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 130/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 133/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 136/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 139/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 141/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 142/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 148/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 154/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 157/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 160/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 162/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 163/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 174/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 187/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 196/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 197/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 199/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18f91629648>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = keras.Input(shape=(8, 7))\n",
    "AI_model = regression_dilated_cnn(model_inputs)\n",
    "print(AI_model.summary())\n",
    "AI_model.compile(loss='mse', optimizer='adam')\n",
    "AI_model.fit(train_x, train_y, epochs=200, batch_size=32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = AI_model.predict(test_x) # test 데이터로 발전량 결과 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dangjin_floating_pred = pred_test.reshape(672, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자재창고 태양광 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_energy=energy[['time','dangjin_warehouse','tmp time']]\n",
    "train_energy=warehouse_energy[~(warehouse_energy['tmp time'].dt.year==2021)].reset_index(drop=True)\n",
    "train_energy=train_energy[24::].reset_index(drop=True) # 2월 29일 예보 데이터가 없음\n",
    "validation_energy=warehouse_energy[warehouse_energy['tmp time'].dt.year==2021].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Input shape : (1036, 8, 7)\n",
      "Train set Label shape : (1036, 24)\n",
      "Validation set Input shape : (31, 8, 7)\n",
      "Validation set Label shape : (31, 24)\n",
      "Test set Input shape : (28, 8, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_x, train_y= data_gen(train_dangjin_fcst,train_energy, 'dangjin_warehouse')\n",
    "idx=(np.isnan(train_x).sum(axis=2).sum(axis=1))==0\n",
    "train_x=train_x[idx]\n",
    "train_y=train_y[idx]\n",
    "shuffle_idx = np.arange(len(train_x))\n",
    "np.random.shuffle(shuffle_idx)\n",
    "train_x = train_x[shuffle_idx]\n",
    "train_y = train_y[shuffle_idx]\n",
    "print('Train set Input shape : ' + str(train_x.shape))\n",
    "print('Train set Label shape : ' + str(train_y.shape))\n",
    "\n",
    "validation_x, validation_y = data_gen(validation_dangjin_fcst,validation_energy, 'dangjin_warehouse')\n",
    "print('Validation set Input shape : ' + str(validation_x.shape))\n",
    "print('Validation set Label shape : ' + str(validation_y.shape))\n",
    "\n",
    "test_x, _ = data_gen(test_dangjin_fcst,validation_energy, 'dangjin_warehouse') # test 데이터 생성\n",
    "print('Test set Input shape : ' + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"regression_dilated_cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 8, 7)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 8, 8)              176       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8)              32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 8, 16)             400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 8, 32)             1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 8, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 8, 64)             6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 8, 64)             4160      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                25650     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                1224      \n",
      "=================================================================\n",
      "Total params: 40,066\n",
      "Trainable params: 39,726\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 2/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 3/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 4/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 5/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 12/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 15/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 18/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 21/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 23/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 33/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 34/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 37/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 56/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 59/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 62/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 64/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 65/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 67/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 73/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 76/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 78/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 79/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 81/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 87/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 93/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 102/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 103/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 107/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 113/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 115/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 118/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 127/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 130/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 133/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 136/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 139/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 141/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 142/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 148/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "33/33 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 154/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 157/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 160/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 162/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 163/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 174/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 187/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 196/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 197/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 199/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18f8c7cf608>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = keras.Input(shape=(8, 7))\n",
    "AI_model = regression_dilated_cnn(model_inputs)\n",
    "print(AI_model.summary())\n",
    "AI_model.compile(loss='mse', optimizer='adam')\n",
    "AI_model.fit(train_x, train_y, epochs=200, batch_size=32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = AI_model.predict(test_x) # test 데이터로 발전량 결과 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dangjin_warehouse_pred = pred_test.reshape(672, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 당진 태양광 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dangjin_energy=energy[['time','dangjin','tmp time']]\n",
    "train_energy=dangjin_energy[~(dangjin_energy['tmp time'].dt.year==2021)].reset_index(drop=True)\n",
    "train_energy=train_energy[24::].reset_index(drop=True) # 2월 29일 예보 데이터가 없음\n",
    "validation_energy=dangjin_energy[dangjin_energy['tmp time'].dt.year==2021].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set Input shape : (1036, 8, 7)\n",
      "Train set Label shape : (1036, 24)\n",
      "Validation set Input shape : (31, 8, 7)\n",
      "Validation set Label shape : (31, 24)\n",
      "Test set Input shape : (28, 8, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "D:\\Anaconda\\envs\\data_engeneering\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_x, train_y= data_gen(train_dangjin_fcst,train_energy, 'dangjin')\n",
    "idx=(np.isnan(train_x).sum(axis=2).sum(axis=1))==0\n",
    "train_x=train_x[idx]\n",
    "train_y=train_y[idx]\n",
    "shuffle_idx = np.arange(len(train_x))\n",
    "np.random.shuffle(shuffle_idx)\n",
    "train_x = train_x[shuffle_idx]\n",
    "train_y = train_y[shuffle_idx]\n",
    "print('Train set Input shape : ' + str(train_x.shape))\n",
    "print('Train set Label shape : ' + str(train_y.shape))\n",
    "\n",
    "validation_x, validation_y = data_gen(validation_dangjin_fcst,validation_energy, 'dangjin')\n",
    "print('Validation set Input shape : ' + str(validation_x.shape))\n",
    "print('Validation set Label shape : ' + str(validation_y.shape))\n",
    "\n",
    "test_x, _ = data_gen(test_dangjin_fcst,validation_energy, 'dangjin') # test 데이터 생성\n",
    "print('Test set Input shape : ' + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[8.00000000e+00, 3.00000000e+00, 2.50000000e+01, ...,\n",
       "         1.50000000e+00, 4.77777778e-01, 3.00000000e+00],\n",
       "        [8.00000000e+00, 6.00000000e+00, 2.40000000e+01, ...,\n",
       "         1.50000000e+00, 4.44444444e-01, 3.00000000e+00],\n",
       "        [8.00000000e+00, 9.00000000e+00, 2.70000000e+01, ...,\n",
       "         9.00000000e-01, 4.63888889e-01, 1.00000000e+00],\n",
       "        ...,\n",
       "        [8.00000000e+00, 1.80000000e+01, 2.70000000e+01, ...,\n",
       "         2.50000000e+00, 8.83333333e-01, 1.00000000e+00],\n",
       "        [8.00000000e+00, 2.10000000e+01, 2.50000000e+01, ...,\n",
       "         2.20000000e+00, 9.00000000e-01, 1.00000000e+00],\n",
       "        [8.00000000e+00, 0.00000000e+00, 2.40000000e+01, ...,\n",
       "         2.20000000e+00, 9.55555556e-01, 1.00000000e+00]],\n",
       "\n",
       "       [[1.10000000e+01, 3.00000000e+00, 7.00000000e+00, ...,\n",
       "         6.20000000e+00, 9.58333333e-01, 1.00000000e+00],\n",
       "        [1.10000000e+01, 6.00000000e+00, 7.00000000e+00, ...,\n",
       "         6.00000000e+00, 9.88888889e-01, 1.00000000e+00],\n",
       "        [1.10000000e+01, 9.00000000e+00, 1.00000000e+01, ...,\n",
       "         4.10000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "        ...,\n",
       "        [1.10000000e+01, 1.80000000e+01, 1.10000000e+01, ...,\n",
       "         3.60000000e+00, 8.97222222e-01, 1.00000000e+00],\n",
       "        [1.10000000e+01, 2.10000000e+01, 8.00000000e+00, ...,\n",
       "         1.80000000e+00, 9.47222222e-01, 1.00000000e+00],\n",
       "        [1.10000000e+01, 0.00000000e+00, 6.00000000e+00, ...,\n",
       "         1.50000000e+00, 6.33333333e-01, 1.00000000e+00]],\n",
       "\n",
       "       [[3.00000000e+00, 3.00000000e+00, 5.00000000e+00, ...,\n",
       "         2.40000000e+00, 9.22222222e-01, 1.00000000e+00],\n",
       "        [3.00000000e+00, 6.00000000e+00, 4.00000000e+00, ...,\n",
       "         1.80000000e+00, 9.83333333e-01, 1.00000000e+00],\n",
       "        [3.00000000e+00, 9.00000000e+00, 8.00000000e+00, ...,\n",
       "         1.60000000e+00, 1.94444444e-02, 1.00000000e+00],\n",
       "        ...,\n",
       "        [3.00000000e+00, 1.80000000e+01, 1.10000000e+01, ...,\n",
       "         2.90000000e+00, 8.30555556e-01, 1.00000000e+00],\n",
       "        [3.00000000e+00, 2.10000000e+01, 7.00000000e+00, ...,\n",
       "         1.40000000e+00, 7.83333333e-01, 1.00000000e+00],\n",
       "        [3.00000000e+00, 0.00000000e+00, 5.00000000e+00, ...,\n",
       "         1.10000000e+00, 5.41666667e-01, 1.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[1.00000000e+01, 3.00000000e+00, 1.00000000e+01, ...,\n",
       "         1.60000000e+00, 5.83333333e-01, 1.00000000e+00],\n",
       "        [1.00000000e+01, 6.00000000e+00, 9.00000000e+00, ...,\n",
       "         1.70000000e+00, 5.69444444e-01, 1.00000000e+00],\n",
       "        [1.00000000e+01, 9.00000000e+00, 1.40000000e+01, ...,\n",
       "         2.00000000e+00, 5.66666667e-01, 1.00000000e+00],\n",
       "        ...,\n",
       "        [1.00000000e+01, 1.80000000e+01, 1.60000000e+01, ...,\n",
       "         2.90000000e+00, 6.80555556e-01, 1.00000000e+00],\n",
       "        [1.00000000e+01, 2.10000000e+01, 1.50000000e+01, ...,\n",
       "         2.30000000e+00, 7.44444444e-01, 1.00000000e+00],\n",
       "        [1.10000000e+01, 0.00000000e+00, 1.30000000e+01, ...,\n",
       "         2.30000000e+00, 8.44444444e-01, 1.00000000e+00]],\n",
       "\n",
       "       [[6.00000000e+00, 3.00000000e+00, 1.70000000e+01, ...,\n",
       "         1.70000000e+00, 5.69444444e-01, 1.00000000e+00],\n",
       "        [6.00000000e+00, 6.00000000e+00, 1.70000000e+01, ...,\n",
       "         1.40000000e+00, 5.00000000e-01, 1.00000000e+00],\n",
       "        [6.00000000e+00, 9.00000000e+00, 2.30000000e+01, ...,\n",
       "         6.00000000e-01, 3.58333333e-01, 1.00000000e+00],\n",
       "        ...,\n",
       "        [6.00000000e+00, 1.80000000e+01, 2.30000000e+01, ...,\n",
       "         3.50000000e+00, 9.13888889e-01, 1.00000000e+00],\n",
       "        [6.00000000e+00, 2.10000000e+01, 1.90000000e+01, ...,\n",
       "         3.00000000e+00, 9.16666667e-01, 1.00000000e+00],\n",
       "        [6.00000000e+00, 0.00000000e+00, 1.80000000e+01, ...,\n",
       "         1.40000000e+00, 9.05555556e-01, 1.00000000e+00]],\n",
       "\n",
       "       [[8.00000000e+00, 3.00000000e+00, 2.00000000e+01, ...,\n",
       "         1.40000000e+00, 7.83333333e-01, 3.00000000e+00],\n",
       "        [8.00000000e+00, 6.00000000e+00, 1.90000000e+01, ...,\n",
       "         1.20000000e+00, 7.63888889e-01, 3.00000000e+00],\n",
       "        [8.00000000e+00, 9.00000000e+00, 2.30000000e+01, ...,\n",
       "         1.30000000e+00, 7.38888889e-01, 4.00000000e+00],\n",
       "        ...,\n",
       "        [8.00000000e+00, 1.80000000e+01, 2.50000000e+01, ...,\n",
       "         2.50000000e+00, 8.30555556e-01, 1.00000000e+00],\n",
       "        [8.00000000e+00, 2.10000000e+01, 2.20000000e+01, ...,\n",
       "         1.40000000e+00, 8.33333333e-01, 1.00000000e+00],\n",
       "        [8.00000000e+00, 0.00000000e+00, 2.00000000e+01, ...,\n",
       "         1.10000000e+00, 8.63888889e-01, 1.00000000e+00]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.        ,  3.        ,  7.        , ...,  3.4       ,\n",
       "          0.575     ,  4.        ],\n",
       "        [ 2.        ,  6.        ,  6.        , ...,  2.5       ,\n",
       "          0.61666667,  4.        ],\n",
       "        [ 2.        ,  9.        ,  6.        , ...,  3.3       ,\n",
       "          0.64444444,  4.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        ,  4.        , ...,  3.6       ,\n",
       "          0.85833333,  3.        ],\n",
       "        [ 2.        , 21.        ,  1.        , ...,  6.6       ,\n",
       "          0.90277778,  3.        ],\n",
       "        [ 2.        ,  0.        , -3.        , ...,  7.4       ,\n",
       "          0.92222222,  1.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        , -5.        , ...,  7.6       ,\n",
       "          0.90833333,  1.        ],\n",
       "        [ 2.        ,  6.        , -6.        , ...,  7.1       ,\n",
       "          0.90555556,  1.        ],\n",
       "        [ 2.        ,  9.        , -6.        , ...,  7.        ,\n",
       "          0.90277778,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        , -4.        , ...,  5.8       ,\n",
       "          0.84444444,  4.        ],\n",
       "        [ 2.        , 21.        , -3.        , ...,  6.2       ,\n",
       "          0.9       ,  4.        ],\n",
       "        [ 2.        ,  0.        , -4.        , ...,  5.4       ,\n",
       "          0.94444444,  4.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        , -5.        , ...,  2.8       ,\n",
       "          0.90277778,  3.        ],\n",
       "        [ 2.        ,  6.        , -6.        , ...,  0.7       ,\n",
       "          0.90555556,  1.        ],\n",
       "        [ 2.        ,  9.        , -4.        , ...,  2.2       ,\n",
       "          0.35833333,  3.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        ,  0.        , ...,  3.1       ,\n",
       "          0.51944444,  4.        ],\n",
       "        [ 2.        , 21.        ,  0.        , ...,  3.5       ,\n",
       "          0.59166667,  4.        ],\n",
       "        [ 2.        ,  0.        ,  2.        , ...,  5.9       ,\n",
       "          0.84722222,  4.        ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.        ,  3.        ,  0.        , ...,  2.3       ,\n",
       "          0.14444444,  1.        ],\n",
       "        [ 2.        ,  6.        ,  0.        , ...,  2.6       ,\n",
       "          0.18055556,  1.        ],\n",
       "        [ 2.        ,  9.        ,  3.        , ...,  4.1       ,\n",
       "          0.24166667,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        , 10.        , ...,  3.8       ,\n",
       "          0.04722222,  1.        ],\n",
       "        [ 2.        , 21.        ,  6.        , ...,  4.5       ,\n",
       "          0.175     ,  1.        ],\n",
       "        [ 2.        ,  0.        ,  3.        , ...,  3.1       ,\n",
       "          0.21388889,  1.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        ,  2.        , ...,  3.7       ,\n",
       "          0.19722222,  1.        ],\n",
       "        [ 2.        ,  6.        ,  1.        , ...,  3.6       ,\n",
       "          0.24444444,  1.        ],\n",
       "        [ 2.        ,  9.        ,  4.        , ...,  4.9       ,\n",
       "          0.27222222,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        , 10.        , ...,  3.7       ,\n",
       "          0.35      ,  1.        ],\n",
       "        [ 2.        , 21.        ,  5.        , ...,  3.3       ,\n",
       "          0.30833333,  1.        ],\n",
       "        [ 2.        ,  0.        ,  3.        , ...,  3.7       ,\n",
       "          0.28055556,  1.        ]],\n",
       "\n",
       "       [[ 2.        ,  3.        ,  1.        , ...,  3.3       ,\n",
       "          0.25555556,  4.        ],\n",
       "        [ 2.        ,  6.        ,  1.        , ...,  3.5       ,\n",
       "          0.325     ,  3.        ],\n",
       "        [ 2.        ,  9.        ,  4.        , ...,  3.6       ,\n",
       "          0.35277778,  1.        ],\n",
       "        ...,\n",
       "        [ 2.        , 18.        ,  9.        , ...,  0.8       ,\n",
       "          0.93611111,  4.        ],\n",
       "        [ 2.        , 21.        ,  7.        , ...,  1.3       ,\n",
       "          0.05      ,  4.        ],\n",
       "        [ 3.        ,  0.        ,  6.        , ...,  1.3       ,\n",
       "          0.33888889,  4.        ]]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"regression_dilated_cnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 8, 7)]            0         \n",
      "_________________________________________________________________\n",
      "conv1d_40 (Conv1D)           (None, 8, 8)              176       \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 8, 8)              32        \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_41 (Conv1D)           (None, 8, 16)             400       \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 8, 16)             64        \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 8, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 8, 32)             1568      \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 8, 32)             128       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_43 (Conv1D)           (None, 8, 64)             6208      \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 8, 64)             256       \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_44 (Conv1D)           (None, 8, 64)             4160      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50)                25650     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 50)                200       \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                1224      \n",
      "=================================================================\n",
      "Total params: 40,066\n",
      "Trainable params: 39,726\n",
      "Non-trainable params: 340\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 2/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 3/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 4/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 5/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 6/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 7/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 8/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 9/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 10/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 11/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 12/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 13/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 14/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 15/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 16/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 17/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 18/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 19/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 20/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 21/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 22/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 23/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 24/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 25/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 26/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 27/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 28/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 29/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 30/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 31/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 32/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 33/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 34/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 35/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 36/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 37/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 38/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 39/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 40/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 41/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 42/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 43/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 44/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 45/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 46/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 47/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 48/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 49/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 50/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 51/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 52/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 53/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 54/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 55/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 56/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 57/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 58/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 59/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 61/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 62/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 63/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 64/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 65/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 66/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 67/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 68/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 69/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 70/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 71/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 72/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 73/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 74/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 75/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 76/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 77/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 78/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 79/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 80/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 81/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 82/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 83/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 84/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 85/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 86/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 87/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 88/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 89/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 90/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 91/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 92/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 93/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 94/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 95/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 96/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 97/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 98/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 99/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 100/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 101/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 102/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 103/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 104/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 105/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 106/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 107/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 108/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 109/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 110/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 111/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 112/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 113/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 114/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 115/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 116/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 117/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 118/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 119/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 120/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 121/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 122/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 123/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 124/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 125/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 126/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 127/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 128/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 129/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 130/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 131/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 132/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 133/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 134/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 135/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 136/200\n",
      "33/33 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 137/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 138/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 139/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 140/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 141/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 142/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 143/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 144/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 145/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 146/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 147/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 148/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 149/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 150/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 151/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 152/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 153/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 154/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 155/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 156/200\n",
      "33/33 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 157/200\n",
      "33/33 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 158/200\n",
      "33/33 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 159/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 160/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 161/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 162/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 163/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 165/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 166/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 167/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 168/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 169/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 170/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 171/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 172/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 173/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 174/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 175/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 176/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 177/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 178/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 179/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 180/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 181/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 182/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 183/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 184/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 185/200\n",
      "33/33 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 186/200\n",
      "33/33 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 187/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 188/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 189/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 190/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 191/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 192/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 193/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 194/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 195/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan: 0s - loss: n\n",
      "Epoch 196/200\n",
      "33/33 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 197/200\n",
      "33/33 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 198/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 199/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 200/200\n",
      "33/33 [==============================] - 0s 4ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18f91fc4948>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = keras.Input(shape=(8, 7))\n",
    "AI_model = regression_dilated_cnn(model_inputs)\n",
    "print(AI_model.summary())\n",
    "AI_model.compile(loss='mse', optimizer='adam')\n",
    "AI_model.fit(train_x, train_y, epochs=200, batch_size=32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = AI_model.predict(test_x) # test 데이터로 발전량 결과 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dangjin_pred = pred_test.reshape(672, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dangjin_floating_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:24*28, 1] = dangjin_floating_pred\n",
    "submission.iloc[:24*28, 2] = dangjin_warehouse_pred\n",
    "submission.iloc[:24*28, 3] = dangjin_pred\n",
    "submission.iloc[:24*28, 4] = ulsan_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>dangjin_floating</th>\n",
       "      <th>dangjin_warehouse</th>\n",
       "      <th>dangjin</th>\n",
       "      <th>ulsan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-01 02:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-01 03:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.004581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-01 04:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-01 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005507</td>\n",
       "      <td>-0.001729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>2021-07-08 20:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>2021-07-08 21:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1389</th>\n",
       "      <td>2021-07-08 22:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>2021-07-08 23:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>2021-07-08 24:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1392 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  dangjin_floating  dangjin_warehouse   dangjin  \\\n",
       "0     2021-02-01 01:00:00               NaN                NaN  0.000017   \n",
       "1     2021-02-01 02:00:00               NaN                NaN -0.000436   \n",
       "2     2021-02-01 03:00:00               NaN                NaN -0.000005   \n",
       "3     2021-02-01 04:00:00               NaN                NaN  0.000700   \n",
       "4     2021-02-01 05:00:00               NaN                NaN  0.005507   \n",
       "...                   ...               ...                ...       ...   \n",
       "1387  2021-07-08 20:00:00               0.0                0.0  0.000000   \n",
       "1388  2021-07-08 21:00:00               0.0                0.0  0.000000   \n",
       "1389  2021-07-08 22:00:00               0.0                0.0  0.000000   \n",
       "1390  2021-07-08 23:00:00               0.0                0.0  0.000000   \n",
       "1391  2021-07-08 24:00:00               0.0                0.0  0.000000   \n",
       "\n",
       "         ulsan  \n",
       "0     0.000051  \n",
       "1     0.000102  \n",
       "2     0.004581  \n",
       "3     0.000029  \n",
       "4    -0.001729  \n",
       "...        ...  \n",
       "1387  0.000000  \n",
       "1388  0.000000  \n",
       "1389  0.000000  \n",
       "1390  0.000000  \n",
       "1391  0.000000  \n",
       "\n",
       "[1392 rows x 5 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engeneering",
   "language": "python",
   "name": "data_engeneering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "7e69553b6b83f67eb397483cec61d6c2efcfcef4a7d9d7451bf16338610db287"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
